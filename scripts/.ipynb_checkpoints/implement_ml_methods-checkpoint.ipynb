{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Change the path according to where is your data \n",
    "DATA_TRAIN_PATH = \"C:\\\\Users\\\\Asus-PC\\\\Desktop\\\\ML\\\\Project1\\\\data\\\\train.csv\" \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform tX by changing -999 with the mean of column\n",
    "def changedNumber(tX, i, j):\n",
    "    mean = 0\n",
    "    nb = 0\n",
    "    for k in range(tX.shape[0]):\n",
    "        if tX[k][j] != -999:\n",
    "            mean += tX[k][j]\n",
    "            nb = nb+1\n",
    "    mean = mean / nb\n",
    "    tX[i][j] = mean\n",
    "    return tX\n",
    "def transformTX(tX):\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if tX[i][j] == -999:\n",
    "                tX = changedNumber(tX, i, j)\n",
    "tX = transformTX(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: We are assuming that we DO NOT have an offset and that w = {w1, w2, ... , wD} where D=30 in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to select a small set of data (Given in lab2)\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss function that we use (this function is not used directly but only its gradient)\n",
    "def compute_loss_lin(y, tX, w):\n",
    "    #Calculate the loss using mse\n",
    "    N = y.shape[0]\n",
    "    e = y - (tX @ w)\n",
    "    return (1/(2*N)) * (e.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the gradient\n",
    "def compute_gradient_lin(y, tX, w):\n",
    "    e = y - (tX @ w)\n",
    "    return (-1/N) * (tX.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient descent algorithm: the function returns best w\n",
    "def least_squares_GD(y, tX, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_lin(y,tX,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stochastic gradient descent algorithm: the function returns best w\n",
    "def least_squares_SGD(y, tX, initial_w, max_iters, gamma):\n",
    "    batch_size = 1\n",
    "    iterate = next(batch_iter(y, tX, batch_size, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tX1 = iterate[1]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_lin(y1,tX1,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Least squares regression using normal equations\n",
    "def least_squares(y, tX):\n",
    "    return np.linalg.solve(tX.T@tX,tX.T@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ridge regression using normal equations \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    N = np.shape(tx)[0]\n",
    "    D = np.shape(tx)[1] \n",
    "    lambda_prime = 2 * N * lambda_\n",
    "    identity = np.eye(D)\n",
    "    LHS = (tx.T@tx) + lambda_prime*identity\n",
    "    RHS = tx.T@y\n",
    "    \n",
    "    return np.linalg.solve(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to compute sigmoid\n",
    "def sigmoid(value): \n",
    "    a = np.exp(-value)\n",
    "    return 1.0 / (1.0 + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the gradient of  \n",
    "def compute_gradient_log(y, tx, w):\n",
    "    return tx.T @ (sigmoid(tx@w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic regression using gradient descent\n",
    "def logistic_regression_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logisitic regression using stochastic gradient descent \n",
    "def logistic_regression_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    batch_size = 1\n",
    "    iterate = next(batch_iter(y, tx, batch_size, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y1, tx1, w)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute regularized gradient\n",
    "def compute_gradient_reg(y, tx, w, lambda_):\n",
    "    return compute_gradient_log(y, tx, w) + lambda_*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized Logistic regression using gradient descent\n",
    "def reg_logistic_regression_GD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y, tx, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized Logisitic regression using stochastic gradient descent \n",
    "def reg_logistic_regression_SGD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    batch_size = 1\n",
    "    iterate = next(batch_iter(t, tx, batch_size, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y1, tx1, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_logistic(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix on a logistic regression\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = sigmoid(y_pred[i])\n",
    "    y_pred[np.where(y_pred < 0.5)] = -1\n",
    "    y_pred[np.where(y_pred >= 0.5)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, gamma, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # ridge regression\n",
    "    initial_w = np.zeros(x.shape[1])\n",
    "    max_iters = 100000\n",
    "    w = logistic_regression_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = compute_loss_lin(y_tr, x_tr, w)\n",
    "    loss_te = compute_loss_lin(y_te, x_te, w)\n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus-PC\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 12\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    gammas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # cross validation\n",
    "    minimum = 10e4\n",
    "    mgamma = 0\n",
    "    for gamma in gammas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te,_ = cross_validation(y, tX, k_indices, k, gamma, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "        if np.mean(rmse_te_tmp) < minimum :\n",
    "            minimum = np.mean(rmse_te_tmp)\n",
    "            mgamma = gamma\n",
    "\n",
    "    cross_validation_visualization(gammas, rmse_tr, rmse_te)\n",
    "    print(mgamma)\n",
    "    print(minimum)\n",
    "#cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'C:\\\\Users\\\\Asus-PC\\\\Desktop\\\\ML\\\\Project1\\\\data\\\\test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'C:\\\\Users\\\\Asus-PC\\\\Desktop\\\\output.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "weights = least_squares(y,tX)\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
