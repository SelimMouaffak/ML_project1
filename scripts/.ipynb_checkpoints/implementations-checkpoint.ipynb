{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T17:53:35.040191Z",
     "start_time": "2021-10-31T17:53:34.041815Z"
    }
   },
   "outputs": [],
   "source": [
    "# Useful starting line\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: We are assuming that we DO NOT have an offset and that w = {w1, w2, ... , wD} where D=30 in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to select a small set of data (Given in lab2)\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss function that we use (this function is not used directly but only its gradient)\n",
    "def compute_loss_lin(y, tX, w):\n",
    "    #Calculate the loss using mse\n",
    "    \n",
    "    e = y - (tX @ w)\n",
    "    return (1/(2*tX.shape[0])) * (e.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.240205Z",
     "start_time": "2021-10-31T14:06:02.164817Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Compute the gradient\n",
    "def compute_gradient_lin(y, tX, w):\n",
    "    e = y - (tX @ w)\n",
    "    return (-1/tX.shape[0]) * (tX.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.324843Z",
     "start_time": "2021-10-31T14:06:02.249236Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Gradient descent algorithm: the function returns best w\n",
    "def least_squares_GD(y, tX, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        grad = compute_gradient_lin(y,tX,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.415616Z",
     "start_time": "2021-10-31T14:06:02.324843Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size_linear = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.512865Z",
     "start_time": "2021-10-31T14:06:02.416801Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Stochastic gradient descent algorithm: the function returns best w\n",
    "def least_squares_SGD(y, tX, initial_w, max_iters, gamma):\n",
    "    iterate = next(batch_iter(y, tX, batch_size_linear, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tX1 = iterate[1]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_lin(y1,tX1,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.602165Z",
     "start_time": "2021-10-31T14:06:02.512865Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Least squares regression using normal equations\n",
    "def least_squares(y, tX):\n",
    "    return np.linalg.solve(tX.T@tX,tX.T@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.674567Z",
     "start_time": "2021-10-31T14:06:02.605817Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Ridge regression using normal equations \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    lambda_prime = 2 * tx.shape[0] * lambda_\n",
    "    identity = np.eye(tx.shape[1])\n",
    "    LHS = (tx.T@tx) + lambda_prime*identity\n",
    "    RHS = tx.T@y\n",
    "    \n",
    "    return np.linalg.solve(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.745053Z",
     "start_time": "2021-10-31T14:06:02.674567Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Helper function to compute sigmoid\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.855635Z",
     "start_time": "2021-10-31T14:06:02.745053Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_logistic_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    val = y.T @ np.log(sigmoid(tx@w)) + (1-y.T) @ np.log(1-sigmoid(tx@w))\n",
    "    return np.squeeze(- val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.931156Z",
     "start_time": "2021-10-31T14:06:02.855635Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Compute the gradient of  \n",
    "def compute_gradient_log(y, tx, w):\n",
    "    return tx.T @ (sigmoid(tx@w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:02.986066Z",
     "start_time": "2021-10-31T14:06:02.931156Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Logistic regression using gradient descent\n",
    "def logistic_regression_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:03.096873Z",
     "start_time": "2021-10-31T14:06:02.987603Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size_log = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:03.185609Z",
     "start_time": "2021-10-31T14:06:03.096873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Logisitic regression using stochastic gradient descent \n",
    "def logistic_regression_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    iterate = next(batch_iter(y, tx, batch_size_log, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y1, tx1, w)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:03.292609Z",
     "start_time": "2021-10-31T14:06:03.186670Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Compute regularized gradient\n",
    "def compute_gradient_reg(y, tx, w, lambda_):\n",
    "    return compute_gradient_log(y, tx, w) + lambda_*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:03.365990Z",
     "start_time": "2021-10-31T14:06:03.295500Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Regularized Logistic regression using gradient descent\n",
    "def reg_logistic_regression_GD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y, tx, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:03.463104Z",
     "start_time": "2021-10-31T14:06:03.365990Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batch_size_reg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T14:06:03.551375Z",
     "start_time": "2021-10-31T14:06:03.467136Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### Regularized Logisitic regression using stochastic gradient descent \n",
    "def reg_logistic_regression_SGD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    iterate = next(batch_iter(t, tx, batch_size_reg, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y1, tx1, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
