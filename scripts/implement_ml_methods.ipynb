{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Change the path according to where is your data \n",
    "DATA_TRAIN_PATH = \"C:\\\\Users\\\\Asus-PC\\\\Desktop\\\\ML\\\\Project1\\\\data\\\\train.csv\" \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0816146888ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0mtX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchangedNumber\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformTX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-0816146888ed>\u001b[0m in \u001b[0;36mtransformTX\u001b[1;34m(tX)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m                 \u001b[0mtX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchangedNumber\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mtX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformTX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-0816146888ed>\u001b[0m in \u001b[0;36mchangedNumber\u001b[1;34m(tX, i, j)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m999\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mmean\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### Transform tX by changing -999 with the mean of column\n",
    "def changedNumber(tX, i, j):\n",
    "    mean = 0\n",
    "    nb = 0\n",
    "    for k in range(tX.shape[0]):\n",
    "        if tX[k][j] != -999:\n",
    "            mean += tX[k][j]\n",
    "            nb = nb+1\n",
    "    mean = mean / nb\n",
    "    tX[i][j] = mean\n",
    "    return tX\n",
    "def transformTX(tX):\n",
    "    for i in range(tX.shape[0]):\n",
    "        for j in range(tX.shape[1]):\n",
    "            if tX[i][j] == -999:\n",
    "                tX = changedNumber(tX, i, j)\n",
    "tX = transformTX(tX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: We are assuming that we DO NOT have an offset and that w = {w1, w2, ... , wD} where D=30 in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to select a small set of data (Given in lab2)\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss function that we use (this function is not used directly but only its gradient)\n",
    "def compute_loss_lin(y, tX, w):\n",
    "    #Calculate the loss using mse\n",
    "    N = y.shape[0]\n",
    "    e = y - (tX @ w)\n",
    "    return (1/(2*N)) * (e.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the gradient\n",
    "def compute_gradient_lin(y, tX, w):\n",
    "    e = y - (tX @ w)\n",
    "    return (-1/N) * (tX.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient descent algorithm: the function returns best w\n",
    "def least_squares_GD(y, tX, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_lin(y,tX,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stochastic gradient descent algorithm: the function returns best w\n",
    "def least_squares_SGD(y, tX, initial_w, max_iters, gamma):\n",
    "    batch_size = 1\n",
    "    iterate = next(batch_iter(y, tX, batch_size, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tX1 = iterate[1]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_lin(y1,tX1,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Least squares regression using normal equations\n",
    "def least_squares(y, tX):\n",
    "    return np.linalg.solve(tX.T@tX,tX.T@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ridge regression using normal equations \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    N = np.shape(tx)[0]\n",
    "    D = np.shape(tx)[1] \n",
    "    lambda_prime = 2 * N * lambda_\n",
    "    identity = np.eye(D)\n",
    "    LHS = (tx.T@tx) + lambda_prime*identity\n",
    "    RHS = tx.T@y\n",
    "    \n",
    "    return np.linalg.solve(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to compute sigmoid\n",
    "def sigmoid(value): \n",
    "    a = np.exp(-value)\n",
    "    return 1.0 / (1.0 + a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the gradient of  \n",
    "def compute_gradient_log(y, tx, w):\n",
    "    return tx.T @ (sigmoid(tx@w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic regression using gradient descent\n",
    "def logistic_regression_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logisitic regression using stochastic gradient descent \n",
    "def logistic_regression_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    batch_size = 1\n",
    "    iterate = next(batch_iter(y, tx, batch_size, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y1, tx1, w)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute regularized gradient\n",
    "def compute_gradient_reg(y, tx, w, lambda_):\n",
    "    return compute_gradient_log(y, tx, w) + lambda_*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized Logistic regression using gradient descent\n",
    "def reg_logistic_regression_GD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y, tx, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized Logisitic regression using stochastic gradient descent \n",
    "def reg_logistic_regression_SGD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    batch_size = 1\n",
    "    iterate = next(batch_iter(t, tx, batch_size, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y1, tx1, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_logistic(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix on a logistic regression\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = sigmoid(y_pred[i])\n",
    "    y_pred[np.where(y_pred < 0.5)] = -1\n",
    "    y_pred[np.where(y_pred >= 0.5)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, gamma, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    # ridge regression\n",
    "    initial_w = np.zeros(x.shape[1])\n",
    "    max_iters = 100000\n",
    "    w = logistic_regression_SGD(y, tX, initial_w, max_iters, gamma)\n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = compute_loss_lin(y_tr, x_tr, w)\n",
    "    loss_te = compute_loss_lin(y_te, x_te, w)\n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    seed = 12\n",
    "    degree = 7\n",
    "    k_fold = 4\n",
    "    gammas = np.logspace(-4, 0, 30)\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    # cross validation\n",
    "    minimum = 10e4\n",
    "    mgamma = 0\n",
    "    for gamma in gammas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te,_ = cross_validation(y, tX, k_indices, k, gamma, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "        if np.mean(rmse_te_tmp) < minimum :\n",
    "            minimum = np.mean(rmse_te_tmp)\n",
    "            mgamma = gamma\n",
    "\n",
    "    cross_validation_visualization(gammas, rmse_tr, rmse_te)\n",
    "    print(mgamma)\n",
    "    print(minimum)\n",
    "#cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = 'C:\\\\Users\\\\Asus-PC\\\\Desktop\\\\ML\\\\Project1\\\\data\\\\test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'C:\\\\Users\\\\Asus-PC\\\\Desktop\\\\output.csv' # TODO: fill in desired name of output file for submission\n",
    "\n",
    "weights = least_squares(y,tX)\n",
    "y_pred = predict_labels(weights, tX_test)\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
