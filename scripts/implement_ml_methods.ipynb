{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "#Change the path according to where is your data \n",
    "DATA_TRAIN_PATH = r\"C:\\Users\\USER\\Desktop\\MA1\\ML\\train.csv\" \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define variables for the dimensions of the data\n",
    "N = tX.shape[0]\n",
    "D = tX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0] == N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transform tX by changing -999 with the mean of column\n",
    "def transformTX(tX):\n",
    "    tX2 = np.copy(tX)\n",
    "    tX2[tX2 == -999] = 0\n",
    "    means = np.mean(tX2, axis=0)\n",
    "    for i in range(N):\n",
    "        for j in range(D):\n",
    "            if tX[i][j] == -999:\n",
    "                tX[i][j] = means[j]\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply log to smoothen data\n",
    "def maybeAddLog(tX):\n",
    "    tX2 = np.copy(tX)\n",
    "    mins = np.min(tX2, axis=0)\n",
    "    for i in range(D):\n",
    "        if mins[i]>0:\n",
    "            for k in range(N):\n",
    "                tX[k][i] = np.log(tX[k][i])\n",
    "    return tX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Features Expansion to capture non linear data\n",
    "def featuresExpansion(tX, degree):\n",
    "    res = np.zeros(N).reshape(-1,1)\n",
    "    for i in range(D):\n",
    "        for d in range(1,degree+1):\n",
    "            col = tX[:,i]**d\n",
    "            col = col.reshape(-1,1)\n",
    "            res = np.hstack((res, col))\n",
    "    res = np.delete(res, 0,1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deg = 6\n",
    "tX = transformTX(tX)\n",
    "tX = maybeAddLog(tX)\n",
    "tX = featuresExpansion(tX, deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: We are assuming that we DO NOT have an offset and that w = {w1, w2, ... , wD} where D=30 in our case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to select a small set of data (Given in lab2)\n",
    "def batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generate a minibatch iterator for a dataset.\n",
    "    Takes as input two iterables (here the output desired values 'y' and the input data 'tx')\n",
    "    Outputs an iterator which gives mini-batches of `batch_size` matching elements from `y` and `tx`.\n",
    "    Data can be randomly shuffled to avoid ordering in the original data messing with the randomness of the minibatches.\n",
    "    \"\"\"\n",
    "    data_size = len(y)\n",
    "\n",
    "    if shuffle:\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        shuffled_y = y[shuffle_indices]\n",
    "        shuffled_tx = tx[shuffle_indices]\n",
    "    else:\n",
    "        shuffled_y = y\n",
    "        shuffled_tx = tx\n",
    "    for batch_num in range(num_batches):\n",
    "        start_index = batch_num * batch_size\n",
    "        end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "        if start_index != end_index:\n",
    "            yield shuffled_y[start_index:end_index], shuffled_tx[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loss function that we use (this function is not used directly but only its gradient)\n",
    "def compute_loss_lin(y, tX, w):\n",
    "    #Calculate the loss using mse\n",
    "    e = y - (tX @ w)\n",
    "    return (1/(2*N)) * (e.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the gradient\n",
    "def compute_gradient_lin(y, tX, w):\n",
    "    e = y - (tX @ w)\n",
    "    return (-1/N) * (tX.T @ e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Gradient descent algorithm: the function returns best w\n",
    "def least_squares_GD(y, tX, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        grad = compute_gradient_lin(y,tX,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_linear = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stochastic gradient descent algorithm: the function returns best w\n",
    "def least_squares_SGD(y, tX, initial_w, max_iters, gamma):\n",
    "    iterate = next(batch_iter(y, tX, batch_size_linear, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tX1 = iterate[1]\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        grad = compute_gradient_lin(y1,tX1,w)\n",
    "        w = w - gamma * grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Least squares regression using normal equations\n",
    "def least_squares(y, tX):\n",
    "    return np.linalg.solve(tX.T@tX,tX.T@y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ridge regression using normal equations \n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    lambda_prime = 2 * N * lambda_\n",
    "    identity = np.eye(D)\n",
    "    LHS = (tx.T@tx) + lambda_prime*identity\n",
    "    RHS = tx.T@y\n",
    "    \n",
    "    return np.linalg.solve(LHS, RHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function to compute sigmoid\n",
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return 1.0 / (1 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\"\"\"\n",
    "    val = y.T @ np.log(sigmoid(tx@w)) + (1-y.T) @ np.log(1-sigmoid(tx@w))\n",
    "    return np.squeeze(- val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute the gradient of  \n",
    "def compute_gradient_log(y, tx, w):\n",
    "    return tx.T @ (sigmoid(tx@w) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logistic regression using gradient descent\n",
    "def logistic_regression_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y, tx, w)\n",
    "        w = w - gamma * gradient\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_log = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Logisitic regression using stochastic gradient descent \n",
    "def logistic_regression_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    iterate = next(batch_iter(y, tx, batch_size_log, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_log(y1, tx1, w)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute regularized gradient\n",
    "def compute_gradient_reg(y, tx, w, lambda_):\n",
    "    return compute_gradient_log(y, tx, w) + lambda_*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized Logistic regression using gradient descent\n",
    "def reg_logistic_regression_GD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    w = initial_w\n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y, tx, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_reg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regularized Logisitic regression using stochastic gradient descent \n",
    "def reg_logistic_regression_SGD(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    iterate = next(batch_iter(t, tx, batch_size_reg, num_batches=1, shuffle=True))\n",
    "    y1 = iterate[0]\n",
    "    tx1 = iterate[1]\n",
    "    w = initial_w\n",
    "    \n",
    "    for _ in range(max_iters):\n",
    "        gradient = compute_gradient_reg(y1, tx1, w, lambda_)\n",
    "        w = w - gamma * gradient\n",
    "        \n",
    "    return w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels_logistic(weights, data):\n",
    "    \"\"\"Generates class predictions given weights, and a test data matrix on a logistic regression\"\"\"\n",
    "    y_pred = np.dot(data, weights)\n",
    "    for i in range(len(y_pred)):\n",
    "        y_pred[i] = sigmoid(y_pred[i])\n",
    "    y_pred[np.where(y_pred < 0.5)] = -1\n",
    "    y_pred[np.where(y_pred >= 0.5)] = 1\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold.\"\"\"\n",
    "    num_row = N\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(y, x, k_indices, k, lambda_, degree):\n",
    "    \"\"\"return the loss of ridge regression.\"\"\"\n",
    "    # get k'th subgroup in test, others in train\n",
    "    te_indice = k_indices[k]\n",
    "    tr_indice = k_indices[~(np.arange(k_indices.shape[0]) == k)]\n",
    "    tr_indice = tr_indice.reshape(-1)\n",
    "    y_te = y[te_indice]\n",
    "    y_tr = y[tr_indice]\n",
    "    x_te = x[te_indice]\n",
    "    x_tr = x[tr_indice]\n",
    "    \n",
    "    # logistic regression\n",
    "    w = ridge_regression(y, tX, lambda_)\n",
    "    \n",
    "    # calculate the loss for train and test data\n",
    "    loss_tr = compute_loss_lin(y_tr, x_tr, w)\n",
    "    loss_te = compute_loss_lin(y_te, x_te, w)\n",
    "    \n",
    "    return loss_tr, loss_te, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining variables\n",
    "seed = 6\n",
    "degree = 7 #Doesn't matter\n",
    "k_fold = 6\n",
    "lambdas = np.logspace(-4, 0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e64caba73f5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mcross_validation_demo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-19-e64caba73f5f>\u001b[0m in \u001b[0;36mcross_validation_demo\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mrmse_te_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_fold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mloss_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_te\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdegree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0mrmse_tr_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mrmse_te_tmp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_te\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-f8fc8784b218>\u001b[0m in \u001b[0;36mcross_validation\u001b[1;34m(y, x, k_indices, k, lambda_, degree)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# get k'th subgroup in test, others in train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mte_indice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtr_indice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtr_indice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_indice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0my_te\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mte_indice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from plots import cross_validation_visualization\n",
    "\n",
    "def cross_validation_demo():\n",
    "    # split data in k fold\n",
    "    k_indices = build_k_indices(y, k_fold, seed)\n",
    "    \n",
    "    # define lists to store the loss of training data and test data\n",
    "    rmse_tr = []\n",
    "    rmse_te = []\n",
    "    \n",
    "    # cross validation\n",
    "    minimum = 100\n",
    "    mlambda = 0\n",
    "    for lambda_ in lambdas:\n",
    "        rmse_tr_tmp = []\n",
    "        rmse_te_tmp = []\n",
    "        for k in range(k_fold):\n",
    "            loss_tr, loss_te,_ = cross_validation(y, tX, k_indices, k, lambda_, degree)\n",
    "            rmse_tr_tmp.append(loss_tr)\n",
    "            rmse_te_tmp.append(loss_te)\n",
    "        rmse_tr.append(np.mean(rmse_tr_tmp))\n",
    "        rmse_te.append(np.mean(rmse_te_tmp))\n",
    "        if np.mean(rmse_te_tmp) < minimum :\n",
    "            minimum = np.mean(rmse_te_tmp)\n",
    "            mlambda = lambda_\n",
    "\n",
    "    cross_validation_visualization(lambdas, rmse_tr, rmse_te)\n",
    "    print(mlambda)\n",
    "    print(minimum)\n",
    "    \n",
    "cross_validation_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate predictions and save ouput in csv format for submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TEST_PATH = r'C:\\Users\\USER\\Desktop\\MA1\\ML\\test.csv' # TODO: download train data and supply path here \n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = r'C:\\Users\\USER\\Desktop\\MA1\\ML\\output.csv' # TODO: fill in desired name of output file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "###IF WE ARE USING LEAST_SQUARES \n",
    "weights = least_squares(y,tX)\n",
    "y_pred = predict_labels(weights, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###IF WE ARE USING LOGISTIC REGRESSION\n",
    "y[y == -1] = 0  #We set y's where it is -1 to 0 in order to work with probabilities\n",
    "initial_w = np.random.randint(-1000, 1000, D)\n",
    "max_iters = 1000\n",
    "gamma = 1e-10\n",
    "weights = logistic_regression_GD(y, tX, initial_w, max_iters, gamma)\n",
    "y_pred = predict_labels_logistic(weights, tX_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create submission\n",
    "create_csv_submission(ids_test, y_pred, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
